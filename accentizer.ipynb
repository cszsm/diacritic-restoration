{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as et\n",
    "\n",
    "vowel_table = {\"a\" : [\"á\"], \"e\" : [\"é\"], \"i\" : [\"í\"], \"o\" : [\"ó\", \"ö\", \"ő\"], \"u\" : [\"ú\", \"ü\", \"ű\"]}\n",
    "vectorizer = DictVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ispunct(c):\n",
    "    punctuations = \"!\\\"#$%&'()*+,-./:;<=>?@[\\\\]^_`{|}~\"\n",
    "    for char in punctuations:\n",
    "        if (c == char):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def isalpha(c):\n",
    "    alphabet = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "    for char in alphabet:\n",
    "        if (c == char):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# reduces the number of different characters to 30\n",
    "def normalize_character(c):\n",
    "    if (c.isspace()):\n",
    "        return ' '\n",
    "    if (c.isdigit()):\n",
    "        return '0'\n",
    "    if (ispunct(c)):\n",
    "        return '_'\n",
    "    if (isalpha(c)):\n",
    "        return c\n",
    "    return '*'\n",
    "\n",
    "\n",
    "def normalize_text(text):\n",
    "    normalized = \"\"\n",
    "    for c in text:\n",
    "        normalized += normalize_character(c)\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def deaccentize(text):\n",
    "    text = text.replace(\"á\", \"a\")\n",
    "    text = text.replace(\"é\", \"e\")\n",
    "    text = text.replace(\"í\", \"i\")\n",
    "    text = text.replace(\"ó\", \"o\")\n",
    "    text = text.replace(\"ö\", \"o\")\n",
    "    text = text.replace(\"ő\", \"o\")\n",
    "    text = text.replace(\"ú\", \"u\")\n",
    "    text = text.replace(\"ü\", \"u\")\n",
    "    text = text.replace(\"ű\", \"u\")\n",
    "\n",
    "    return text\n",
    "\n",
    "def create_row(window, window_size):\n",
    "    row = {}\n",
    "\n",
    "    for i in range(-window_size, window_size + 1):\n",
    "        row[i] = normalize_character(deaccentize(window.popleft()))\n",
    "\n",
    "    del row[0]\n",
    "\n",
    "    return row\n",
    "\n",
    "def get_y(vowel):\n",
    "    if vowel in \"aei\":\n",
    "        return [1, 0]\n",
    "    if vowel in \"áéí\":\n",
    "        return [0, 1]\n",
    "    if vowel in \"ou\":\n",
    "        return [1, 0, 0, 0]\n",
    "    if vowel in \"óú\":\n",
    "        return [0, 1, 0, 0]\n",
    "    if vowel in \"öü\":\n",
    "        return [0, 0, 1, 0]\n",
    "    if vowel in \"őű\":\n",
    "        return [0, 0, 0, 1]\n",
    "\n",
    "# returns\n",
    "# x_e: list of windows with the given window_size\n",
    "# y_e: one-hot encoded values\n",
    "def prepare_text(text, window_size, vowel):\n",
    "    x_e = []\n",
    "    y_e = []\n",
    "    lower_text = text.lower()\n",
    "\n",
    "    window = deque((), window_size * 2 + 1)\n",
    "    for i in range(window.maxlen):\n",
    "        window.append(\"_\")\n",
    "        lower_text += \"_\"\n",
    "\n",
    "    for character in lower_text:\n",
    "        window.append(character)\n",
    "        \n",
    "        if (window[window_size] == vowel) or (window[window_size] in vowel_table[vowel]):\n",
    "            x_e.append(create_row(window.copy(), window_size))\n",
    "            y_e.append(get_y(window[window_size]))\n",
    "\n",
    "    return x_e, y_e\n",
    "\n",
    "def prepare_words(words, window_size, vowel):\n",
    "    x_e = []\n",
    "    y_e = []\n",
    "    \n",
    "    for word in words:\n",
    "        skip = True\n",
    "        if vowel in word:\n",
    "            skip = False\n",
    "        for c in vowel_table[vowel]:\n",
    "            if c in word:\n",
    "                skip = False\n",
    "        if skip:\n",
    "            continue\n",
    "            \n",
    "        x, y = prepare_text(word, window_size, vowel)\n",
    "\n",
    "        if x_e == []:\n",
    "            x_e = vectorizer.transform(x).toarray()\n",
    "        else:\n",
    "            tmp = vectorizer.transform(x)\n",
    "            for tx in tmp:\n",
    "                #x_e += tx.toarray()\n",
    "                x_e = np.concatenate((x_e, tx.toarray()))\n",
    "        y_e += y\n",
    "        \n",
    "    return x_e, y_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generates template windows for the alphabet\n",
    "def generate_windows(window_size):\n",
    "    windows = []\n",
    "    alphabet = \"abcdefghijklmnopqrstuvwxyz 0_*\"\n",
    "    alphabet_size = len(alphabet)\n",
    "\n",
    "    for i in range(alphabet_size):\n",
    "        new_window = {}\n",
    "\n",
    "        end_of_slice = i + window_size * 2\n",
    "        if end_of_slice <= alphabet_size:\n",
    "            alphabet_slice = alphabet[i:end_of_slice]\n",
    "        else:\n",
    "            alphabet_slice = alphabet[i:alphabet_size]\n",
    "            alphabet_slice += alphabet[0:end_of_slice - alphabet_size]\n",
    "\n",
    "        for j in range(window_size):\n",
    "            new_window[-1 * (j + 1)] = alphabet_slice[window_size - 1 - j]\n",
    "            new_window[j + 1] = alphabet_slice[window_size + j]\n",
    "\n",
    "        windows.append(new_window)\n",
    "\n",
    "    return windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_corpus(count):\n",
    "    corpus = open(\"corpus\")\n",
    "    words = []\n",
    "    \n",
    "    for i in range(4):\n",
    "        next(corpus)\n",
    "        \n",
    "    for line in corpus:\n",
    "        splits = line.split()\n",
    "        if splits != []:\n",
    "            words.append(splits[0])\n",
    "            if count < 0:\n",
    "                break\n",
    "            count -= 1\n",
    "        \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,\n",
       "        sparse=True)"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.fit(generate_windows(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zscseh93/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel/__main__.py:76: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n"
     ]
    }
   ],
   "source": [
    "count = 10000\n",
    "words = read_corpus(count)\n",
    "\n",
    "vowel = \"e\"\n",
    "p_x, p_y = prepare_words(words, 4, vowel)\n",
    "np.savez(\"prepared_\" + vowel, x = p_x, y = p_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def next_batch(data_x, data_y, count):\n",
    "    batch_x, batch_y = [], []\n",
    "    indexes = random.sample(range(len(data_x)), count)\n",
    "    \n",
    "    for i in indexes:\n",
    "        batch_x.append(data_x[i])\n",
    "        batch_y.append(data_y[i])\n",
    "        \n",
    "    return batch_x, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def convert_time(time):\n",
    "    m, s = divmod(time, 60)\n",
    "    s, ms = divmod(s, 1)\n",
    "\n",
    "    return \"%02d:%02d:%04d\" % (m, s, ms * 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_size = 240\n",
    "output_size = 2\n",
    "\n",
    "n_input = tf.placeholder(tf.float32, [None, input_size])\n",
    "n_output = tf.placeholder(tf.float32, [None, output_size])\n",
    "\n",
    "hidden_neurons = 10\n",
    "\n",
    "b_hidden = tf.Variable(tf.random_normal([hidden_neurons]))\n",
    "W_hidden = tf.Variable(tf.random_normal([input_size, hidden_neurons]))\n",
    "hidden = tf.sigmoid(tf.matmul(n_input, W_hidden) + b_hidden)\n",
    "\n",
    "W_output = tf.Variable(tf.random_normal([hidden_neurons, output_size]))\n",
    "# output = tf.sigmoid(tf.matmul(hidden, W_output))\n",
    "output = tf.nn.softmax(tf.matmul(hidden, W_output))\n",
    "\n",
    "# cost = tf.reduce_mean(tf.square(n_output - output))\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(n_output * tf.log(output), reduction_indices=[1]))\n",
    "\n",
    "# optimizer = tf.train.GradientDescentOptimizer(0.5)\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training started\n",
      "5382\n",
      "2307\n",
      "5382\n",
      "\n",
      "step:   0\n",
      "loss: 0.9193103313446045\n",
      "|||||||||||||||||||\n",
      "step: 1000\n",
      "loss: 0.4019467830657959\n",
      "|||||||||||||||||||\n",
      "step: 2000\n",
      "loss: 0.31510597467422485\n",
      "\n",
      "elapsed time: 03:07:0725\n",
      "best loss: 0.22418\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "print(\"training started\")\n",
    "\n",
    "# data_train = np.load(\"train_e.npz\")\n",
    "# data_test = np.load(\"test_e.npz\")\n",
    "\n",
    "prepared_data = np.load(\"prepared_e.npz\")\n",
    "train_x, test_x, train_y, test_y = train_test_split(prepared_data[\"x\"], prepared_data[\"y\"], test_size = 0.3)\n",
    "print(len(train_x))\n",
    "print(len(test_x))\n",
    "print(len(train_y))\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "loss = 100\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "for i in range(2001):\n",
    "    batch_x, batch_y = next_batch(train_x, train_y, 100)\n",
    "    cvalues = sess.run([train, cost, W_hidden, b_hidden, W_output], feed_dict={n_input: batch_x, n_output: batch_y})\n",
    "#     cvalues = sess.run([train, cost, W_hidden, b_hidden, W_output], feed_dict={n_input: data_train[\"x\"], n_output: data_train[\"y\"]})\n",
    "\n",
    "    # early stopping\n",
    "    if i % 50 == 0 and cvalues[1] < loss:\n",
    "        loss = cvalues[1]\n",
    "        saver.save(sess, \"session\")\n",
    "#         print(\"a best loss: {}\".format(cvalues[1]))\n",
    "        \n",
    "\n",
    "#     if i < 101 and i % 100 == 0:\n",
    "#         print(\"\")\n",
    "#         print(\"\")\n",
    "#         print(\"step: {:>3}\".format(i))\n",
    "#         print(\"loss: {}\".format(cvalues[1]))\n",
    "    if i % 1000 == 0:\n",
    "        print(\"\")\n",
    "        print(\"step: {:>3}\".format(i))\n",
    "        print(\"loss: {}\".format(cvalues[1]))\n",
    "    elif i % 50 == 0:\n",
    "        print('|', end=\"\")\n",
    "\n",
    "print(\"\")\n",
    "print(\"elapsed time: \" + convert_time(time.perf_counter() - start_time))\n",
    "print(\"best loss: \" + str(loss))\n",
    "\n",
    "result = sess.run(output, feed_dict={n_input: test_x})\n",
    "saver.restore(sess, \"session\")\n",
    "result_best = sess.run(output, feed_dict={n_input: test_x})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.857824013870828\n",
      "accuracy (best): 0.8573905504984829\n",
      "5382\n",
      "2307\n"
     ]
    }
   ],
   "source": [
    "def decide(y):\n",
    "#     tmp = [0, 0, 0, 0]\n",
    "    tmp = [0, 0]\n",
    "    tmp[list(y).index(max(y))] = 1\n",
    "    return tmp\n",
    "#     if y[0] > y[1]:\n",
    "#         return np.array([1, 0])\n",
    "#     else:\n",
    "#         return np.array([0, 1])\n",
    "    \n",
    "success = 0\n",
    "\n",
    "for i in range(len(result)):\n",
    "    if np.array_equal(decide(result[i]), test_y[i]):\n",
    "        success += 1\n",
    "        \n",
    "print(\"accuracy: \" + str(success / len(result)))\n",
    "\n",
    "\n",
    "success = 0\n",
    "\n",
    "for i in range(len(result_best)):\n",
    "    if np.array_equal(decide(result_best[i]), test_y[i]):\n",
    "        success += 1\n",
    "        \n",
    "print(\"accuracy (best): \" + str(success / len(result_best)))\n",
    "print(len(train_x))\n",
    "print(len(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1], [0, 1], [1, 0], [1, 0]]\n"
     ]
    }
   ],
   "source": [
    "test_data = \"Zsarnoki törvény és erkölcsi zsarnokság egyaránt nyomasztó.\"\n",
    "tmp_x, test_y = prepare_text(test_data, 4, \"e\")\n",
    "test_x = vectorizer.transform(tmp_x).toarray()\n",
    "\n",
    "print(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1]\n",
      "[0, 1]\n",
      "[1, 0]\n",
      "[1, 0]\n"
     ]
    }
   ],
   "source": [
    "result = sess.run(output, feed_dict={n_input: test_x})\n",
    "\n",
    "for i in result:\n",
    "    print(decide(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
